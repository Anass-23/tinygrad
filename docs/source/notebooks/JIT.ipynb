{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JIT Compilation\n",
    "\n",
    "Additionally, it is possible to speed up the computation of certain neural networks by using the JIT.\n",
    "Currently, this does not support models with varying input sizes and non tinygrad operations.\n",
    "\n",
    "To use the JIT (Just in Time) compilation we just need to add a function decorator to the forward pass of our neural network and ensure that the input and output are realized tensors.\n",
    "Ã¥Or in this case we will create a wrapper function and decorate the wrapper function to speed up the evaluation of our neural network.\n",
    "\n",
    "## Tinynet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fetch_mnist import fetch_mnist\n",
    "from tinygrad.helpers import Timing\n",
    "from tinygrad.tensor import Tensor\n",
    "\n",
    "class Linear:\n",
    "  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n",
    "    self.weight = getattr(Tensor, initialization)(out_features, in_features)\n",
    "    self.bias = Tensor.zeros(out_features) if bias else None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return x.linear(self.weight.transpose(), self.bias)\n",
    "\n",
    "class TinyNet:\n",
    "  def __init__(self):\n",
    "    self.l1 = Linear(784, 128, bias=False)\n",
    "    self.l2 = Linear(128, 10, bias=False)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.l1(x)\n",
    "    x = x.leakyrelu()\n",
    "    x = self.l2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.103640625\n",
      "Time: 3123.10 ms\n"
     ]
    }
   ],
   "source": [
    "from tinygrad.jit import TinyJit\n",
    "\n",
    "net = TinyNet()\n",
    "X_train, Y_train, X_test, Y_test = fetch_mnist()\n",
    "\n",
    "@TinyJit\n",
    "def jit(x):\n",
    "  return net(x).realize()\n",
    "\n",
    "with Timing(\"Time: \"):\n",
    "  avg_acc = 0\n",
    "  for step in range(1000):\n",
    "    # random sample a batch\n",
    "    samp = np.random.randint(0, X_test.shape[0], size=(64))\n",
    "    batch = Tensor(X_test[samp], requires_grad=False)\n",
    "    # get the corresponding labels\n",
    "    labels = Y_test[samp]\n",
    "\n",
    "    # forward pass with jit\n",
    "    out = jit(batch)\n",
    "\n",
    "    # calculate accuracy\n",
    "    pred = out.argmax(axis=-1).numpy()\n",
    "    avg_acc += (pred == labels).mean()\n",
    "  print(f\"Test Accuracy: {avg_acc / 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find that the evaluation time is much faster than before and that your accelerator utilization is much higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
