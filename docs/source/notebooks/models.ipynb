{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks in tinygrad are really just represented by the operations performed on tensors.\n",
    "These operations are commonly grouped into the `__call__` method of a class which allows modularization and reuse of these groups of operations.\n",
    "These classes do not need to inherit from any base class, in fact if they don't need any trainable parameters they don't even need to be a class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tinygrad.helpers import Timing\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.helpers import dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of this would be the `nn.Linear` class which represents a linear layer in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n",
    "    self.weight = getattr(Tensor, initialization)(out_features, in_features)\n",
    "    self.bias = Tensor.zeros(out_features) if bias else None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return x.linear(self.weight.transpose(), self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more neural network modules already implemented in [nn](/tinygrad/nn/__init__.py), and you can also implement your own.\n",
    "\n",
    "We will be implementing a simple neural network that can classify handwritten digits from the MNIST dataset.\n",
    "Our classifier will be a simple 2 layer neural network with a Leaky ReLU activation function.\n",
    "It will use a hidden layer size of 128 and an output layer size of 10 (one for each digit) with no bias on either Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet:\n",
    "  def __init__(self):\n",
    "    self.l1 = Linear(784, 128, bias=False)\n",
    "    self.l2 = Linear(128, 10, bias=False)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.l1(x)\n",
    "    x = x.leakyrelu()\n",
    "    x = self.l2(x)\n",
    "    return x\n",
    "\n",
    "net = TinyNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the forward pass of our neural network is just the sequence of operations performed on the input tensor `x`.\n",
    "We can also see that functional operations like `leakyrelu` are not defined as classes and instead are just methods we can just call.\n",
    "Finally, we just initialize an instance of our neural network, and we are ready to start training it.\n",
    "\n",
    "## Training\n",
    "\n",
    "Now that we have our neural network defined we can start training it.\n",
    "Training neural networks in tinygrad is super simple.\n",
    "All we need to do is define our neural network, define our loss function, and then call `.backward()` on the loss function to compute the gradients.\n",
    "They can then be used to update the parameters of our neural network using one of the many optimizers in [optim.py](/tinygrad/nn/optim.py).\n",
    "\n",
    "For our loss function we will be using sparse categorical cross entropy loss. The implementation below is taken from [tensor.py](../../tinygrad/tensor.py), it's copied below to highlight an important detail of tinygrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_categorical_crossentropy(self, Y, ignore_index=-1) -> Tensor:\n",
    "    loss_mask = Y != ignore_index\n",
    "    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n",
    "    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n",
    "    return self.log_softmax().mul(y).sum() / loss_mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this implementation of cross entropy loss, there are certain operations that tinygrad does not support natively.\n",
    "Namely, operations that are load/store or assigning a value to a tensor at a certain index.\n",
    "Load/store ops are not supported in tinygrad natively because they add complexity when trying to port to different backends, 90% of the models out there don't use/need them, and they can be implemented like it's done above with an `arange` mask.\n",
    "\n",
    "For our optimizer we will be using the traditional stochastic gradient descent optimizer with a learning rate of 3e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.nn.optim import SGD\n",
    "\n",
    "opt = SGD([net.l1.weight, net.l2.weight], lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are passing in the parameters of our neural network to the optimizer.\n",
    "This is due to the fact that the optimizer needs to know which parameters to update.\n",
    "There is a simpler way to do this just by using `get_parameters(net)` from `tinygrad.nn.state` which will return a list of all the parameters in the neural network.\n",
    "The parameters are just listed out explicitly here for clarity.\n",
    "\n",
    "Now that we have our network, loss function, and optimizer defined all we are missing is the data to train on!\n",
    "There are a couple of dataset loaders in tinygrad located in [/extra/datasets](/extra/datasets).\n",
    "We will be using the MNIST dataset loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip, tarfile, pickle\n",
    "from tinygrad.helpers import fetch\n",
    "\n",
    "def fetch_mnist(tensors=False):\n",
    "  parse = lambda file: np.frombuffer(gzip.open(file).read(), dtype=np.uint8).copy()\n",
    "  BASE_URL = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"   # http://yann.lecun.com/exdb/mnist/ lacks https\n",
    "  X_train = parse(fetch(f\"{BASE_URL}train-images-idx3-ubyte.gz\"))[0x10:].reshape((-1, 28*28)).astype(np.float32)\n",
    "  Y_train = parse(fetch(f\"{BASE_URL}train-labels-idx1-ubyte.gz\"))[8:]\n",
    "  X_test = parse(fetch(f\"{BASE_URL}t10k-images-idx3-ubyte.gz\"))[0x10:].reshape((-1, 28*28)).astype(np.float32)\n",
    "  Y_test = parse(fetch(f\"{BASE_URL}t10k-labels-idx1-ubyte.gz\"))[8:]\n",
    "  if tensors: return Tensor(X_train).reshape(-1, 1, 28, 28), Tensor(Y_train), Tensor(X_test).reshape(-1, 1, 28, 28), Tensor(Y_test)\n",
    "  else: return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to start training our neural network.\n",
    "We will be training for 1000 steps with a batch size of 64.\n",
    "\n",
    "We use `with Tensor.train()` set the internal flag `Tensor.training` to `True` during training.\n",
    "Upon exit, the flag is restored to its previous value by the context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss: 162.09005737304688 | Accuracy: 0.078125\n",
      "Step 101 | Loss: 6.010622978210449 | Accuracy: 0.796875\n",
      "Step 201 | Loss: 4.344995498657227 | Accuracy: 0.84375\n",
      "Step 301 | Loss: 5.003681659698486 | Accuracy: 0.875\n",
      "Step 401 | Loss: 2.147794246673584 | Accuracy: 0.890625\n",
      "Step 501 | Loss: 3.499648094177246 | Accuracy: 0.84375\n",
      "Step 601 | Loss: 3.2069578170776367 | Accuracy: 0.921875\n",
      "Step 701 | Loss: 2.139812707901001 | Accuracy: 0.921875\n",
      "Step 801 | Loss: 5.188339710235596 | Accuracy: 0.8125\n",
      "Step 901 | Loss: 0.40168359875679016 | Accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = fetch_mnist()\n",
    "\n",
    "with Tensor.train():\n",
    "  for step in range(1000):\n",
    "    # Random sample a batch\n",
    "    samp = np.random.randint(0, X_train.shape[0], size=(64))\n",
    "    batch = Tensor(X_train[samp], requires_grad=False)\n",
    "    # Get the corresponding labels\n",
    "    labels = Tensor(Y_train[samp])\n",
    "\n",
    "    # Forward pass\n",
    "    out = net(batch)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = sparse_categorical_crossentropy(out, labels)\n",
    "\n",
    "    # Zero gradients\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    opt.step()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    pred = out.argmax(axis=-1)\n",
    "    acc = (pred == labels).mean()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      print(f\"Step {step+1} | Loss: {loss.numpy()} | Accuracy: {acc.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have trained our neural network we can evaluate it on the test set.\n",
    "We will be using the same batch size of 64 and will be evaluating for 1000 of those batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.894109375\n",
      "Time: 4893.06 ms\n"
     ]
    }
   ],
   "source": [
    "with Timing(\"Time: \"):\n",
    "  avg_acc = 0\n",
    "  for step in range(1000):\n",
    "    # Random sample a batch\n",
    "    samp = np.random.randint(0, X_test.shape[0], size=(64))\n",
    "    batch = Tensor(X_test[samp], requires_grad=False)\n",
    "\n",
    "    # Get the corresponding labels\n",
    "    labels = Y_test[samp]\n",
    "\n",
    "    # Forward pass\n",
    "    out = net(batch)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    pred = out.argmax(axis=-1).numpy()\n",
    "    avg_acc += (pred == labels).mean()\n",
    "  print(f\"Test Accuracy: {avg_acc / 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "The standard weight format for tinygrad is [safetensors](https://github.com/huggingface/safetensors). This means that you can load the weights of any model also using safetensors into tinygrad.\n",
    "There are functions in [state.py](/tinygrad/nn/state.py) to save and load models to and from this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  0.00 GB, l2.weight                                         : 100%|██████████| 2/2 [00:00<00:00, 50.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 138.58 ms, 0.00 GB loaded at 0.01 GB/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tinygrad.nn.state import safe_save, safe_load, get_state_dict, load_state_dict\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "# We need to make sure the directory exists\n",
    "pathlib.Path(\"./tmp/models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# First we need the state dict of our model\n",
    "state_dict = get_state_dict(net)\n",
    "\n",
    "# Then we can just save it to a file\n",
    "safe_save(state_dict, \"./tmp/models/model_tinynet.safetensors\")\n",
    "\n",
    "# And load it back in\n",
    "state_dict = safe_load(\"./tmp/models/model_tinynet.safetensors\")\n",
    "load_state_dict(net, state_dict)\n",
    "\n",
    "# Remove the file (example purposes)\n",
    "shutil.rmtree(\"./tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the models in the [models/](/models) folder have a `load_from_pretrained` method that will download and load the weights for you. These usually are pytorch weights meaning that you would need pytorch installed to load them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Computation Graph\n",
    "\n",
    "It is possible to visualize the computation graph of a neural network using [graphviz](https://graphviz.org/).\n",
    "\n",
    "This is easily done by running a single pass (forward or backward!) of the neural network with the environment variable `GRAPH` set to `1`.\n",
    "The graph will be saved to `/tmp/net.svg` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "\n",
    "And that's it!\n",
    "\n",
    "We highly recommend you check out the [examples/](/examples) folder for more examples of using tinygrad.\n",
    "Reading the source code of tinygrad is also a great way to learn how it works.\n",
    "Specifically the tests in [test/](/test) are a great place to see how to use and the semantics of the different operations.\n",
    "There are also a bunch of models implemented in [models/](/models) that you can use as a reference.\n",
    "\n",
    "Additionally, feel free to ask questions in the `#learn-tinygrad` channel on the [Discord](https://discord.gg/beYbxwxVdx). Don't ask to ask, just ask!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
